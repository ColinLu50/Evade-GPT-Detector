import argparse
import os.path

import numpy as np
import openai
from tqdm import tqdm
import torch
import pathlib


import shared_dir
import copy
import multiprocessing





from my_utils.data_utils import save_list_to_tsv, load_list_from_tsv
from my_utils.test_utils import get_ai_probs, get_score_saving_dir, find_threshold, get_ai_perc_threshold, get_auc
from my_utils.my_dataloader import load_test_output_ai, load_test_output_human



# for k in args:
#     print(k, args[k])

def _run_detection_cache(dataset, method_name, detector_name, text_dir, device, use_cache):
    _, score_dir = get_score_saving_dir(dataset=dataset, gen_method=method_name, detector=detector_name)
    if use_cache and score_dir.exists():
        data_list = load_list_from_tsv(score_dir)
        score_list = [float(d[1]) for d in data_list]
        print('Load detection result from', score_dir)
        return score_list

    if method_name == 'orig':
        text_list = load_test_output_ai(dataset)
    elif method_name == 'human':
        text_list = load_test_output_human(dataset)
    else:
        pair_text_list = load_list_from_tsv(text_dir, skip_header=True)
        text_list = [d[1] for d in pair_text_list]

    print(f'========= Dataset:{dataset} Method:{method_name} Detector:{detector_name} ==============')
    ai_prob_list = get_ai_probs(text_list, detector_name, dataset, device, detector_cache=True)

    text_score_list = zip(text_list, ai_prob_list)
    print('Save detection result to', score_dir)
    save_list_to_tsv(text_score_list, score_dir)

    return ai_prob_list


def get_auc_by_ai_human_score(ai_score_list, human_score_list):
    y_true = np.array([1 for _ in range(len(ai_score_list))] + [0 for _ in range(len(human_score_list))])
    y_score = ai_score_list + human_score_list
    auc = get_auc(y_true, y_score)

    return auc


def run_detection(args):
    # check text generated by methods
    text_dir = f"./outputs/test_results/{args.dataset}/{args.method}/generated_text.tsv"

    if not os.path.exists(text_dir):
        raise Exception(f'{args.method} generated text does not exist in {text_dir}')

    # check if detector score exists
    score_list = _run_detection_cache(dataset=args.dataset, method_name=args.method, detector_name=args.detector,
                              text_dir=text_dir, device=args.device, use_cache=args.use_cache)

    score_list_ai = _run_detection_cache(dataset=args.dataset, method_name='orig', detector_name=args.detector,
                              text_dir=text_dir, device=args.device, use_cache=args.use_cache)

    score_list_human = _run_detection_cache(dataset=args.dataset, method_name='human', detector_name=args.detector,
                              text_dir=text_dir, device=args.device, use_cache=args.use_cache)

    assert len(score_list_ai) == len(score_list_human)

    if len(score_list) < len(score_list_ai):
        test_len = len(score_list)
        score_list_ai = score_list_ai[:test_len]
        score_list_human = score_list_human[:test_len]

    return score_list, score_list_ai, score_list_human



def get_all_metrics(args, score_list, score_list_ai, score_list_human, tag=''):
    low_FPR_value = 0.05
    high_TPR_value = 0.95

    print(f'SICO: {len(score_list)}, AI: {len(score_list_human)}, Human: {len(score_list_human)}')



    # =================== ACC =====================
    # get thresholds
    low_FPR_threshold = find_threshold(score_list_human, low_FPR_value)
    high_TPR_threshold = find_threshold(score_list_ai, high_TPR_value)


    # human text
    human_low_FPR_ai_perc = get_ai_perc_threshold(score_list_human, low_FPR_threshold)
    human_high_TPR_ai_perc = get_ai_perc_threshold(score_list_human, high_TPR_threshold)


    # orig AI text
    ai_low_FPR_ai_perc = get_ai_perc_threshold(score_list_ai, low_FPR_threshold)
    ai_high_TPR_ai_perc = get_ai_perc_threshold(score_list_ai, high_TPR_threshold)


    low_FPR_ai_perc = get_ai_perc_threshold(score_list, low_FPR_threshold)
    high_TPR_ai_perc = get_ai_perc_threshold(score_list, high_TPR_threshold)


    # ================= AUC ===============================
    method_auc = get_auc_by_ai_human_score(score_list, score_list_human)
    orig_auc = get_auc_by_ai_human_score(score_list_ai, score_list_human)



    msg_ = f'''
    Dataset: {args.dataset}, Detector: {args.detector}
    =============== ACC ==================
    Low Threshold: {high_TPR_threshold: .5f}
    AI: {ai_high_TPR_ai_perc:.2%}
    {args.method}: {high_TPR_ai_perc:.2%}
    Human(FPR): {human_high_TPR_ai_perc:.2%}
    --------------------------------------
    
    High Threshold: {low_FPR_threshold: .5f}
    AI: {ai_low_FPR_ai_perc:.2%}
    {args.method}: {low_FPR_ai_perc:.2%}
    Human(FPR): {human_low_FPR_ai_perc:.2%}
    --------------------------------------
    
    ================ AUC ==================
    Orig: {orig_auc:.4f}
    {args.method}: {method_auc: .4f}
    '''

    print(msg_)








if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    parser.add_argument("--dataset", required=True, type=str, choices=["squad", "eli5", "yelp"],
            help="dataset to be tested")
    parser.add_argument("--method", required=True, type=str, help="generation method of AI texts")
    parser.add_argument("--detector", required=True, type=str,
                        choices=['chatdetect', 'gpt2detect', 'logrank', 'openai', 'detectgpt', 'gptzero'],
                        help="detector to be used")

    parser.add_argument("--device", default=0, type=int,
            help="device number")
    parser.add_argument("--use-cache", action='store_true')


    args = parser.parse_args()


    # test by detector
    score_list, score_list_ai, score_list_human = run_detection(args)

    # get metric: AUC and accuracy
    get_all_metrics(args, score_list, score_list_ai, score_list_human)



